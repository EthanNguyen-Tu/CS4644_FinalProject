{"cells":[{"cell_type":"markdown","metadata":{"id":"msqMETf9v8BQ"},"source":["# CS 4644: The Perceptrons"]},{"cell_type":"markdown","metadata":{"id":"UclH4IOAiCOC"},"source":["## Part 1: Setup\n","\n","It is a good idea to run all cells in this section if you want to run any other cells in this notebook from Google Colab."]},{"cell_type":"markdown","metadata":{"id":"Mfi8AGpbaMdv"},"source":["##### STEP 1: Mount Google Drive for Google Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14526,"status":"ok","timestamp":1744180639051,"user":{"displayName":"Ethan","userId":"05453352028402182029"},"user_tz":240},"id":"98Dh2KUzaLzy","outputId":"ee40f2dd-f236-4fa8-e6a9-a2e645c4358e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iq1fFuJ2ZBPa"},"outputs":[],"source":["\n","drive_path = \"drive/MyDrive\" # NOTE: Separated so that colab can access the '.kaggle' folder in your Google Drive for Kaggle API authentication\n","group_folder = drive_path + \"/CS4644_ThePerceptrons\""]},{"cell_type":"markdown","metadata":{"id":"WchPaj4nsHV6"},"source":["##### STEP 2: Basic Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vki7EovtFxoz"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import zipfile\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["##### STEP 3: Helper Functions"],"metadata":{"id":"T1RCFPwOxOZT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uue85utAU-II"},"outputs":[],"source":["def zip_to_colab(zip_file_path, extract_dir_name):\n","  extract_dir = '/content/' + extract_dir_name + \"/\"\n","  os.makedirs(extract_dir, exist_ok=True)\n","\n","  with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","      zip_ref.extractall(extract_dir)\n","\n","  print(f\"Files from {zip_file_path} extracted to: {extract_dir}\")\n","  print(\"Number of files extracted:\", len(os.listdir(extract_dir)))"]},{"cell_type":"markdown","metadata":{"id":"lVSX2a4wFShw"},"source":["## PART 2: Load the Datasets"]},{"cell_type":"markdown","source":["##### STEP 1: Load the \"Human Faces\" Dataset\n","\n","Dataset Source: [\"Human Faces\" by Ashwin Gupta](https://www.kaggle.com/datasets/ashwingupta3012/human-faces/data)"],"metadata":{"id":"yNIzKI3mTyaX"}},{"cell_type":"code","source":["zip_to_colab(group_folder + '/HumanFacesReduced.zip', \"HumanFacesImages\")\n","HUMANFACES_IMAGE_EXTENSIONS = {'.png', '.JPG', '.jpeg', '.jpg'}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8r1Y-1XJUBEv","executionInfo":{"status":"ok","timestamp":1744181453291,"user_tz":240,"elapsed":8762,"user":{"displayName":"Ethan","userId":"05453352028402182029"}},"outputId":"927c297c-cb45-4025-ad07-0a814f7d2e21"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files from drive/MyDrive/CS4644_ThePerceptrons/HumanFacesReduced.zip extracted to: /content/HumanFacesImages/\n","Number of files extracted: 3273\n"]}]},{"cell_type":"markdown","source":["##### STEP 2: Load the \"Fake-Vs-Real-Faces (Hard)\" Dataset\n","\n","Dataset Source: [\"Fake-Vs-Real-Faces (Hard)\" by Hamza Boulahi](https://www.kaggle.com/datasets/hamzaboulahia/hardfakevsrealfaces)"],"metadata":{"id":"imnIk_fNUG2d"}},{"cell_type":"code","source":["zip_to_colab(group_folder + '/RealImages.zip', 'TestRealImages')\n","zip_to_colab(group_folder + '/FakeImages.zip', 'TestFakeImages')\n","FAKEVREAL_IMAGE_EXTENSIONS = {'.jpg'}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UZ86uVi5T6SV","executionInfo":{"status":"ok","timestamp":1744181455502,"user_tz":240,"elapsed":351,"user":{"displayName":"Ethan","userId":"05453352028402182029"}},"outputId":"1f0f1a5f-c24e-4333-db14-ccb3d4dfa9c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files from drive/MyDrive/CS4644_ThePerceptrons/RealImages.zip extracted to: /content/TestRealImages/\n","Number of files extracted: 589\n","Files from drive/MyDrive/CS4644_ThePerceptrons/FakeImages.zip extracted to: /content/TestFakeImages/\n","Number of files extracted: 700\n"]}]},{"cell_type":"markdown","metadata":{"id":"JwwTutWZv-Mp"},"source":["## PART 3: Define the Models"]},{"cell_type":"markdown","metadata":{"id":"cip2eBLs6djD"},"source":["##### STEP 1: Reference Paper Keras Model\n","\n","Original Paper Reference: https://philarchive.org/archive/SALCOR-3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6169,"status":"ok","timestamp":1744180662042,"user":{"displayName":"Ethan","userId":"05453352028402182029"},"user_tz":240},"id":"LrRqOaEe6lH2","outputId":"796e86ce-995c-49da-ab1d-6a7568b5781c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}],"source":["# Package Imports\n","from keras import layers\n","from keras import models\n","from keras import backend as K\n","\n","# The Reference Model\n","reference_model = models.Sequential()\n","reference_model.add(layers.Conv2D(32, (3, 3), activation='relu',\n","  input_shape=(256, 256, 3)))\n","reference_model.add(layers.MaxPooling2D((2, 2)))\n","reference_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","reference_model.add(layers.MaxPooling2D((2, 2)))\n","reference_model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","reference_model.add(layers.MaxPooling2D((2, 2)))\n","reference_model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n","reference_model.add(layers.MaxPooling2D((2, 2)))\n","reference_model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n","reference_model.add(layers.MaxPooling2D((2, 2)))\n","reference_model.add(layers.Conv2D(512, (3, 3), activation='relu'))\n","reference_model.add(layers.MaxPooling2D((2, 2)))\n","reference_model.add(layers.Flatten())\n","reference_model.add(layers.Dense(512, activation='relu'))\n","reference_model.add(layers.Dense(2, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"81-Mx1pH6gOZ"},"source":["##### STEP 2: PyTorch Conversion\n","\n","Convert the Reference Paper Keras Model to PyTorch"]},{"cell_type":"code","source":["# Package Imports\n","import torch\n","import torch.nn as nn\n","\n","# The Reference Model Converted\n","# TODO: Verify the parameters given to each of the layers below.\n","\n","conv2d_ks = 3 # Conv2d Kernel Size\n","conv2d_pad = 1 # Conv2d Padding\n","conv2d_s = 1 # Conv2d Stride\n","\n","maxpool2d_ks = 2 # MaxPool2d Kernel Size\n","maxpool2d_s = 2 # MaxPool2d Stride\n","\n","my_model = nn.Sequential(\n","    nn.Conv2d(3, 32, kernel_size=conv2d_ks, padding=conv2d_pad, stride=conv2d_s),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=maxpool2d_ks, stride=maxpool2d_s),\n","    nn.Conv2d(32, 64, kernel_size=conv2d_ks, padding=conv2d_pad, stride=conv2d_s),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=maxpool2d_ks, stride=maxpool2d_s),\n","    nn.Conv2d(64, 128, kernel_size=conv2d_ks, padding=conv2d_pad, stride=conv2d_s),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=maxpool2d_ks, stride=maxpool2d_s),\n","    nn.Conv2d(128, 256, kernel_size=conv2d_ks, padding=conv2d_pad, stride=conv2d_s),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=maxpool2d_ks, stride=maxpool2d_s),\n","    nn.Conv2d(256, 256, kernel_size=conv2d_ks, padding=conv2d_pad, stride=conv2d_s),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=maxpool2d_ks, stride=maxpool2d_s),\n","    nn.Conv2d(256, 512, kernel_size=conv2d_ks, padding=conv2d_pad, stride=conv2d_s),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=maxpool2d_ks, stride=maxpool2d_s),\n","    nn.Flatten(),\n","    nn.Linear(8192, 512),\n","    nn.ReLU(),\n","    nn.Linear(512, 1),\n",")"],"metadata":{"id":"JtCmGZ1HUmiU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6ke4uY5-6eP"},"source":["##### STEP 3: PyTorch Model Optimizations\n","\n","We optimize the PyTorch Conversion model to achieve better scores and classify images as real or fake, and then classify the fake images as synthetic or deepfake."]},{"cell_type":"markdown","source":["**Model 1**\n","\n","Optimizations:\n","\n","\n","*   List item\n","*   List item\n","\n"],"metadata":{"id":"1R0OQHgRzLCm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zn7UWxCS-7zJ"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["**Model 2**\n","\n","Optimizations:\n","\n","*   List item\n","*   List item\n"],"metadata":{"id":"aoSfcXowzaGq"}},{"cell_type":"code","source":[],"metadata":{"id":"mfbHjPjUzh_f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Model 3**\n","\n","Optimizations:\n","\n","*   List item\n","*   List item"],"metadata":{"id":"AhY0hXHjziSK"}},{"cell_type":"code","source":[],"metadata":{"id":"NfVMccPBznSy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PART 4: Establishing the Baseline"],"metadata":{"id":"7A6b263Fy-Lh"}},{"cell_type":"markdown","source":["##### STEP 1: Train the Keras Model and PyTorch Conversion\n","\n","Goal: Confirm that the Keras Model and PyTorch Conversion have the same accuracy on both the test and train set."],"metadata":{"id":"txOKAww2zw_J"}},{"cell_type":"markdown","source":["Hyperparameters:"],"metadata":{"id":"I62c16m4Nar1"}},{"cell_type":"markdown","source":["##### STEP 2: Baseline Visualization\n","\n","Compares the Keras Model and PyTorch Conversion results."],"metadata":{"id":"S06Qv5jt0Aff"}},{"cell_type":"markdown","source":["Untrained Model Comparison"],"metadata":{"id":"oJLfFgp7NsyY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DUyj5TCKOqte"},"outputs":[],"source":["# Accuracy Values\n","acc_keras =\n","acc_torch =\n","\n","plt.figure(figsize=(6, 4))\n","plt.bar([\"Keras\", \"PyTorch\"], [acc_keras, acc_torch], color=[\"purple\", \"orange\"])\n","plt.ylim(0, 100)\n","plt.ylabel(\"Accuracy (%)\")\n","plt.title(\"Model Accuracy\")\n","plt.grid(axis=\"y\")\n","plt.show()"]},{"cell_type":"markdown","source":["Trained Model Loss and Validation Accuracy"],"metadata":{"id":"zdnJbIkhNZGX"}},{"cell_type":"code","source":["# Data for epochs, loss and validation accuracy\n","epochs = list(range(1, 11))\n","loss = []\n","val_acc = []\n","\n","\n","fig, ax = plt.subplots()\n","\n","# Plot Loss\n","ax.set_xlabel('Epoch')\n","ax.set_ylabel('Loss', color='tab:red')\n","ax.plot(epochs, loss, color='tab:red', label='Loss')\n","ax.tick_params(axis='y', labelcolor='tab:red')\n","\n","# Plot Validation Accuracy\n","ax = ax.twinx()\n","ax.set_xlabel('Epoch')\n","ax.set_ylabel('Validation Accuracy', color='tab:blue')\n","ax.plot(epochs, val_acc, color='tab:blue', label='Validation Accuracy')\n","ax.tick_params(axis='y', labelcolor='tab:blue')\n","\n","# Title and Layout\n","plt.suptitle('Loss and Validation Accuracy over Epochs')\n","fig.tight_layout()\n","\n","# Show plot\n","plt.show()"],"metadata":{"id":"dENe-I_eMQEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Visualizaiton of Original PyTorch vs. Optimized Model 3"],"metadata":{"id":"0QzyyheU093g"}},{"cell_type":"markdown","source":["## PART 5: Original PyTorch vs. Optimized Models"],"metadata":{"id":"L-XHyT17zDw5"}},{"cell_type":"markdown","source":["##### STEP 1: vs. Model 1"],"metadata":{"id":"F4kN0Luc0oNd"}},{"cell_type":"markdown","source":["1. Training of Optimized Model 1"],"metadata":{"id":"X4HshreI0xeM"}},{"cell_type":"code","source":[],"metadata":{"id":"DUKR1Lkt0vk9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Visualizaiton of Original PyTorch vs. Optimized Model 1"],"metadata":{"id":"Gyq9RVuv04iJ"}},{"cell_type":"code","source":[],"metadata":{"id":"nY2-bCMW0-pg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### STEP 2: vs. Model 2"],"metadata":{"id":"ZCQCaAT-0sbU"}},{"cell_type":"markdown","source":["1. Training of Optimized Model 2"],"metadata":{"id":"V0IlYCwb01xA"}},{"cell_type":"code","source":[],"metadata":{"id":"v8XEHXZO03mt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Visualizaiton of Original PyTorch vs. Optimized Model 2"],"metadata":{"id":"hTJXrhBb0877"}},{"cell_type":"code","source":[],"metadata":{"id":"8b7txdkV0-_w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### STEP 3: vs. Model 3"],"metadata":{"id":"HDCKER3H0tuQ"}},{"cell_type":"markdown","source":["1. Training of Optimized Model 3"],"metadata":{"id":"zkRYnBDw02n6"}},{"cell_type":"code","source":[],"metadata":{"id":"RtHd-9mb036O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Visualizaiton of Original PyTorch vs. Optimized Model 3"],"metadata":{"id":"3dw7T6fq3wxK"}},{"cell_type":"code","source":[],"metadata":{"id":"03qsdfQQ3xxz"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["UclH4IOAiCOC","Mfi8AGpbaMdv","T1RCFPwOxOZT","cip2eBLs6djD","81-Mx1pH6gOZ"],"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}